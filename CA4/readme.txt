We came in from CA4 only missing a few tests, and were already reasonably faster than the optimized reference compiler. Strangely, those few tests we were missing were fixed by implementing more optimizing features. Our main additions were: more rigorous Int/Bool unboxing, switching to 32-bit arithmetic, some minor peephole optimizations, improvements to our calling convention, rewriting of our internals, and data-flow analysis.

Unboxing was improved so that Ints and Bools were only boxed when necessary. We managed to reduce this to case expressions, assignment into variables of type Object, and branch merges (with ifs or cases) where the least upper bound was Object. All other instances, including function parameters, local variables, and object attributes, were left unboxed. We implemented the unboxing by adding a flag to our virtual register objects indicating whether the value was boxed or unboxed, and essentially pulled values out of their containing objects wherever possible. Inconsistencies with unboxing turned out to be the source of most of our errors. At one point a new error (on the CA5 server) showed up in the middle of implementing the extended unboxing, and it was strange to discover that the solution was to keep going rather than fixing something that was already written.

Our arithmetic for CA4 involved some strange bit shifting to get 64-bit instructions to match the function of 32-bit instructions. This led to three assembly instructions for what should have been one on each arithmetic operation. We fixed this flaw by switching to 32-bit instructions, adding some extra logic to handle using the appropriate size register for each instruction.

Our peephole optimizations focused on eliminating some dumb code patterns that showed up in the translation from TAC to assembly that we could not really eliminate any other way. One of the bigger ones was an "if (a < b)" condition check taking six instructions rather than the required two. Reducing that case should have improved loop performance, and showed a small increase in speed.

We improved our calling convention to match the C calling convention. For CA4 our easy-to-implement convention was that everything gets passed on the stack, and all registers are callee save. We modified our register allocation to have conflicts with the caller save registers on virtual registers that were live during a function call. We also changed our references to parameters to resolve to the appropriate registers first before going to the stack. By matching the C calling convention, we were able to directly use assembly code generated by gcc from C code for our internals (before we had to modify the assembly to follow our calling convention), which made rewriting our internals much easier. It also allowed us to use heavier optimization from gcc on our internals code as we no longer needed to be able to read the assembly.

It seems that loading libraries takes far longer than expected. By doing some profiling, we discovered that the first call to a function from a library (e.g. from including strings.h for strlen or stdlib.h for malloc) took around 50% of the program's runtime, presumably from looking up or loading the shared library code. By rewriting our internals to minimize the number of libraries used, we cut our average runtime in half (from ~50% to ~30%). The major removals were string utility functions and malloc. String utility functions were simply inlined wherever they occurred, and malloc was replaced with a custom memory management function. The custom memory management used sbrk, a wrapper around a system call that claims memory (still technically a library function, but for some reason takes less time to load), to allocate large blocks that were then filled in order whenever memory was needed.

Our data-flow analysis implementation used a lattice with three levels: Bottom represents storage (i.e. virtual registers) that we have not reached yet, Mid represents storage whose value we know, and Top represents storage whose value we do not know. In addition, we designated data types for each storage; the value of the storage when we are at Mid depends on the type. For Int, the value is a two-tuple representing a possible range (i.e. lower bound and upper bound) that the int could be in (we did not end up making use of the range concept but we kept it in case we had time). For Bool, the value is a raw bool; similarly String holds a raw string. For Object, the value is a two-tuple -- the first element is an isvoid flag (a bool), and the second element is the dynamic type (a string). With this lattice designed, our transfer function can do integer and boolean arithmetic, precompute string functions, and analyze case branches and isvoid statements for constant propagation. Our join function does a least upper bound to move up the lattice. We execute global data-flow by initializing all basic blocks to have all virtual registers at Bottom and try repeating local data-flow until we reach a fixed point; in case this process takes too long, we enforce a timeout.

There are two main benefits to data-flow for our compiler. First, we can precompute conditional branches and change the TAC so that they are unconditional -- the hope is that this cuts down on branch misprediction for the hardware. Second, we can replace method arguments with constants when possible, and use our existing dead code removal to get rid of the operations that led up to the arguments originally being created. These enhancements are pretty nifty for small, simple programs, but they are trickier to deal with when class attributes are involved (plus, data-flow itself takes much longer for large programs) -- for this reason we apply these optimizations conservatively, and the corresponding speedup on the CA5 server is only marginal.

We ended up with about twenty more tests for CA5, and included some of the more interesting tests. The first test is what reproduced most of the bugs we were seeing on the test server but were having trouble finding on our own. Many of the issues involved Int function parameters being unboxed, but local variables being boxed, causing strange things when parameters were re-assigned to. The second test is what we imagined the mat5 test from the server looks like; we wrote it because it was one of our last failing tests and we thought it might be useful for benchmarking. It didn't reproduce the error we were seeing, but it was useful for finding inefficiencies. The third test is a simple test meant to stress case statements. The fourth test is a very large object meant to stress our memory management.

Although we pass all the tests on the CA5 server and all of our own tests, we noticed that some (~5) of the tests on the CA4 server fail! It is likely that these failures come from taking shortcuts in our internal IO implementation and not covering all the corner cases like we did before. In any case, this goes to show that having a comprehensive test suite for a compiler is a difficult task, and that it is very tough to make optimizations truly semantics preserving.

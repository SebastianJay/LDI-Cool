For this assignment, we used python and ply to build our parser. The main phases were: reading in the token stream, parsing and building the abstract syntax tree, and serializing the abstract syntax tree.

Since ply typically expects to perform both the lexing and parsing steps, we built a dummy lexer that read in the .cl-lex file and fed each token in sequence to the parser. Ply allows any object to act as a lexer provided it has a token() method that returns a LexToken object (which consists of token type, lexeme, line number, and column position) -- making such an object was simple given the format of the .cl-lex file. We had some issues with reading the token file in a way that preserved the whitespace in string constants, but eventually settled on an implementation that read in all types of newlines as the \n character and stripped that character from the end of the line.

We took the formal grammar for Cool from the reference manual and reformatted it for ply. The "*" and "+" constructs from that grammar were reimplemented as right recursive grammar rules of the form "list -> e list | empty". Each rule for ply was associated with a function; within those functions we built the abstract syntax tree. For each rule we returned one of the AST node objects from our AST serializer/deserializer code that we shared with our CA2 implementation. For example, in an expression rule of the form "expression -> expression plus expression", we returned an ASTExpression node object of type "plus" with two other ASTExpression objects (returned recursively from ply) as child nodes.

After translating the grammar rules from the CRM into ply, we specified precedence and associativity rules. Ply had a straightforward format for these rules -- we inserted symbols in ascending order of precedence into a list, coupling them with certain strings to denote whether those symbols were right, left, or non-associative.

With our first iteration complete, our grammar only produced 8 shift/reduce conflicts and 12 reduce/reduce conflicts. All of the shift/reduce conflicts came from the "let" expression, which has an expression at the end that can start new derivations. The CRM did not specify a precedence for "let", but commonsense (and testing) told us that the parser should shift in all cases (since reducing early would defeat the purpose of creating the local variable). Luckily, ply resolves all shift/reduce conflicts by shifting, so we were good to go. All of the reduce/reduce conflicts came from ambiguity in seeing a dot after derivations of expressions that had another expression at the end. In this case, the parser was confused whether it should reduce the expression it had derived so far or the empty production that indicated that static dispatch was not being used (i.e. seeing the dot means no "at identifier"). Since dot has high precedence, we resolved this case in favor of the empty production -- by bringing it to the top of the file, ply knew to choose it every time.

In hindsight, our code for serializing and deserializing an abstract syntax tree was rather verbose, but we were able to use it for both CA2 and PA3 which saved time on debugging and rewriting common code. Our implementation used a set of objects representing types of nodes in the AST with fields holding the children of that node. Our root was an AST object which held a list of ASTClass objects and so on. This structure allowed us to build the tree using simple constructors that took arguments from the ply rules, and a simple (to use) recursive load method that deserialized the tree from the lines of a file. The most complicated part of the code was the ASTExpression class which dealt with all the different types of expressions. We could have implemented expressions using a polymorphic structure, but that would have made the code even more verbose than it already was. Instead, we indicated the type of expression with a string field, and stored the varying sub-parts of each expression in a general "args" field which made use of Python's dynamic typing. Our load method for expressions used a large if-else tree to handle the many types of expressions (grouping expressions with similar arguments together to use the same code), and during parsing we simply fed the appropriate tuple of objects to the constructor. Serializing the tree used an overridden __str__ method that followed the rules from the assignment specification. This let us serialize the tree by simply calling str(ast).

Our good test case attempts to stress as many parts of the parsing process as possible. It contains at least one of every construct present in the grammar, and several tests of precedence and associativity rules. Our bad test case looks at a rather convoluted case expression, although the actual error is a missing semicolon, which should be the most frequent error seen by the parser due to how unintuitive semicolon usage is in Cool.

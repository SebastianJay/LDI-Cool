For this assignment, we used python and ply to build our parser. The main phases were: reading in the token stream, parsing and building the abstract syntax tree, and serializing the abstract syntax tree.

Since ply typically expects to perform both the lexing and parsing steps, we built a dummy lexer that simply read in the .cl-lex file and fed each token in sequence to the parser. We had some issues with reading the token file in a way that kept whitespace string constants, but eventually settled on a functional implementation. **dummy lexer stuff**

We took the formal grammar for cool from the reference manual and reformatted it for ply. The "*" and "+" constructs from that grammar where re-implemented as right recursive grammar rules of the form "list -> e list | empty". Each rule for ply was associated with a function, so there we built the abstract syntax tree. For each rule we returned one of the AST node objects from our AST serializer/deserializer code that we shared with our CA2 implementation. For example, an expression rule of the form "expression -> expression plus expression" we returned an ASTExpression object of type "plus" with two other ASTExpression objects (returned recursively from ply) as sub-parts.

In hindsight, our code for serializing and deserializing an abstract syntax tree was rather verbose, but we were able to use it for both CA2 and PA3 which saved time on debugging and rewriting common code. Our implementation used a set of objects representing types of nodes in the AST with fields holding the children of that node. Our root was an AST object which held a list of ASTClass objects and so on. This structure allowed us to build the tree using simple constructors that took arguments from the ply rules, and a simple (to use) recursive load method that deserialized the tree from the lines of a file. The most complicated part of the code was the ASTExpression class which dealt with all the different types of expressions. We could have implemented expressions using a polymorphic structure, but that would have been even more verbose than the code already was. Instead we indicated the type of expression with a string field, and stored the varying sub-parts of each expression in a general "args" field which made use of python's dynamic typing. Our load method for expressions used a large if-else tree to handle the many types of expressions (grouping expressions with similar arguments into groups to use the same code), and during parsing we simply fed the appropriate tuple of objects to the constructor. Serializing the tree used an overridden __str__ method that followed the rules from the assignment statement. This let us serialize the tree by simply calling str(ast).

Our good test case attempts to stress as many parts of the parsing process as possible. It contains at least one of every construct present in the grammar, and several tests of precedence and associativity rules. Our bad test case looks at a rather convoluted case expression, although the actual error is a missing semicolon which should clearly be the most tested error from the parser due to its frequency of appearance while writing cool code.
